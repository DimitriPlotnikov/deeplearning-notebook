{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77e7f1f-1b1e-4fe2-8f03-fb901813da63",
   "metadata": {},
   "source": [
    "# Neural Bag of Words\n",
    "<img src=\"imgs/01.nbow-intro.png\" width=\"800\" height=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d675dac1-85a8-4141-a65f-826956caf68d",
   "metadata": {},
   "source": [
    "## Why NBoW?\n",
    "- **A classic “Hello World” task for NLP.**  \n",
    "  Neural Bag-of-Words (NBoW) provides a simple yet effective baseline model for text classification, making it an ideal starting point for understanding core NLP concepts.\n",
    "\n",
    "- **Introduction to sentiment analysis.**  \n",
    "  In this tutorial, we build a machine learning model for *sentiment analysis*, a fundamental subtask of text classification.  \n",
    "  The objective is to determine whether a given sentence or review expresses **positive** or **negative** sentiment.\n",
    "\n",
    "- **Real-world dataset: IMDb movie reviews.**  \n",
    "  The model is trained and evaluated on the widely used Internet Movie Database (IMDb) dataset, which contains labeled movie reviews.\n",
    "\n",
    "- **Foundational NLP concepts.**  \n",
    "  Along the way, essential techniques are introduced and explained, including:\n",
    "  - Tokenization  \n",
    "  - Vocabulary construction  \n",
    "  - Numerical encoding of text  \n",
    "  - Word embeddings  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40956ea4-f438-41e7-9093-1c0af5bfc79b",
   "metadata": {},
   "source": [
    "## Global and imports settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6130e226-3e8e-4e6e-af96-3ac0c1372d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3585929370>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import collections\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f0ce3c-9bd2-4bd8-8581-10cc395fd06e",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "- **Load the dataset** using the `datasets` library.\n",
    "- **Split the dataset** into training and test sets.\n",
    "- **Inspect the data structure:**\n",
    "  - Each sample consists of:\n",
    "    - `text`: the movie review\n",
    "    - `label`: the sentiment label  \n",
    "      - `0` → negative review  \n",
    "      - `1` → positive review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b48e9b-fd78-40cb-a6e4-e22c43351f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw, test_data_raw = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "train_data_raw, test_data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc3017b2-b106-4f21-8308-6f47e0cc5188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65b0869-9552-47a6-84e8-9f8ec94f3687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c676dba7-2bef-46c1-880a-5d695ee4bb09",
   "metadata": {},
   "source": [
    "## Building input tokenizer\n",
    "* Tokenizers split unstructured raw text into manageable units (words, subwords, or characters) that models can understand.\n",
    "* word_tokenize is a function from Natural Language Toolkit's (NLTK) tokenize module that splits text into individual words and punctuation tokens, e.g., words, commas, periods for further NLP processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f98cc13-c9c5-4bdb-8c1b-631aa96e7061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer:  ['Hello', ',', 'Mr.', 'Smith', '!', 'Is', \"n't\", 'NLP', 'cool', '?']\n",
      "Split:  ['Hello,', 'Mr.', 'Smith!', \"Isn't\", 'NLP', 'cool?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "def get_tokens(text):\n",
    "    return word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "\n",
    "text = \"Hello, Mr. Smith! Isn't NLP cool?\"\n",
    "print(\"Tokenizer: \", word_tokenize(text))  \n",
    "print(\"Split: \", text.split()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856b27c-e913-4b63-b8ac-508088f923a0",
   "metadata": {},
   "source": [
    "## Applying Tokenizer: Mapping input text to tokens\n",
    "* Preprocess data: apply tokenizer to training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec96824-d378-43d1-b630-d67b5fd196e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "def tokenize_input(input, max_length):\n",
    "    tokens = get_tokens(input[\"text\"])[:max_length] # truncate text if too long\n",
    "    return {\"tokens\": tokens}\n",
    "    \n",
    "train_data_tok = train_data_raw.map(\n",
    "    tokenize_input, fn_kwargs={\"max_length\": max_length}\n",
    ")\n",
    "test_data_tok = test_data_raw.map(\n",
    "    tokenize_input, fn_kwargs={\"max_length\": max_length}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb66595-7702-4512-8ed0-965d6af18e8f",
   "metadata": {},
   "source": [
    "* **tokenize_input** is a helper function that performs tokenization of text data from the dataset using the specified tokenizer. Its primary purposes are:\n",
    "  * Tokenize Text: Converts raw text (from the dataset) into tokens\n",
    "  * Truncating longer sequences to fit within max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee4f996-64fd-4a91-8dba-06bcbc3ec474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'tokens'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f0d4af-5b6b-42bc-ab4e-8618cda52ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tok.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e75aa8-b219-4042-b628-8261b6d88197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'rented',\n",
       " 'i',\n",
       " 'am',\n",
       " 'curious-yellow',\n",
       " 'from',\n",
       " 'my',\n",
       " 'video',\n",
       " 'store',\n",
       " 'because',\n",
       " 'of',\n",
       " 'all',\n",
       " 'the',\n",
       " 'controversy',\n",
       " 'that',\n",
       " 'surrounded',\n",
       " 'it',\n",
       " 'when',\n",
       " 'it',\n",
       " 'was',\n",
       " 'first',\n",
       " 'released',\n",
       " 'in',\n",
       " '1967.',\n",
       " 'i']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tok[0][\"tokens\"][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759f912-46ae-4a74-bf7b-e42e50b09041",
   "metadata": {},
   "source": [
    "## Building Vocabulary\n",
    "\n",
    "* *Numerical Foundation*: Neural networks process numbers (tensors), not raw text.\n",
    "* *NBow Vocabulary Mapping*: Maps each unique token to a unique integer index.\n",
    "* *Corpus Analysis*: Builds the vocabulary by calculating token frequencies across the training set.\n",
    "* *Generalization via Filtering*: Removes rare tokens (below min_freq) to reduce noise and prevent overfitting.\n",
    "* *Special Tokens*: Integrates reserved tokens for <UNK> (out-of-vocabulary) and <PAD> (sequence length normalization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f07d2a6b-03b2-41ed-84e6-0b4445fe07cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 25607\n",
      "['themselves', 'jr.', 'how', 'filmmakers', 'hard', 'seems', 'awfully', 'sloppy', 'saw', 'glimpse']\n"
     ]
    }
   ],
   "source": [
    "# Building Vocabulary: Mapping word → unique ID\n",
    "all_tokens = [token for example in train_data_tok for token in example[\"tokens\"]]\n",
    "word_counts = Counter(all_tokens)  # Count word occurrences\n",
    "\n",
    "# Assign unique IDs (starting from 2 to reserve 0 for PAD, 1 for UNK)\n",
    "min_freq = 5\n",
    "\n",
    "word_counts_filtered = {x: count for x, count in word_counts.items() if count >= min_freq}\n",
    "\n",
    "vocab = {word: idx + 2 for idx, word in enumerate(word_counts_filtered) }\n",
    "pad_token_index = 0\n",
    "unknown_token_index = 1\n",
    "vocab[\"[PAD]\"] = pad_token_index  # Padding token\n",
    "vocab[\"[UNK]\"] = unknown_token_index  # Unknown token\n",
    "\n",
    "# Print vocabulary size\n",
    "print(f\"Vocabulary Size: {len(vocab)}\")\n",
    "\n",
    "# Print first 10 words in vocabulary\n",
    "print(list(vocab.keys())[500:510])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226802e-0bb7-4a91-9131-e55df23b23ee",
   "metadata": {},
   "source": [
    "## Apply vocablurary to text tokens\n",
    "* create a numerical vector from tokenized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9d08e09-1b54-4070-9132-ecf7d319a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_dataset(example, vocab):\n",
    "    # Use UNK for unknown words\n",
    "    ids = [vocab.get(token, unknown_token_index) for token in example[\"tokens\"]] \n",
    "    return {\"ids\": ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5f8c349-4a58-4b43-912d-10e0a0d9f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_num = train_data_tok.map(numericalize_dataset, fn_kwargs={\"vocab\": vocab})\n",
    "test_data_num = test_data_tok.map(numericalize_dataset, fn_kwargs={\"vocab\": vocab})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5be85ce-570c-4444-a40c-00dcd0575c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "\n",
      "Tokens:\n",
      "['i', 'rented', 'i', 'am', '[UNK]', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'was', 'first', 'released', 'in', '1967.', 'i', 'also', 'heard', 'that', 'at', 'first', 'it', 'was', 'seized', 'by', 'u.s.', 'customs', 'if', 'it', 'ever', 'tried', 'to', 'enter', 'this', 'country', ',', 'therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', '``', 'controversial', \"''\", 'i', 'really', 'had', 'to', 'see', 'this', 'for', 'myself.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'plot', 'is', 'centered', 'around', 'a', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life', '.', 'in', 'particular', 'she', 'wants', 'to', 'focus', 'her', 'attentions', 'to', 'making', 'some', 'sort', 'of', 'documentary', 'on', 'what', 'the', 'average', '[UNK]', 'thought', 'about', 'certain', 'political', 'issues', 'such', 'as', 'the', 'vietnam', 'war', 'and', 'race', 'issues', 'in', 'the', 'united', 'states', '.', 'in', 'between', 'asking', 'politicians', 'and', 'ordinary', 'denizens', 'of', 'stockholm', 'about', 'their', 'opinions', 'on', 'politics', ',', 'she', 'has', 'sex', 'with', 'her', 'drama', 'teacher', ',', 'classmates', ',', 'and', 'married', 'men.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'what', 'kills', 'me', 'about', 'i', 'am', '[UNK]', 'is', 'that', '40', 'years', 'ago', ',', 'this', 'was', 'considered', 'pornographic', '.', 'really', ',', 'the', 'sex', 'and', 'nudity', 'scenes', 'are', 'few', 'and', 'far', 'between', ',', 'even', 'then', 'it', \"'s\", 'not', 'shot', 'like', 'some', 'cheaply', 'made', 'porno', '.', 'while', 'my', '[UNK]', 'mind', 'find', 'it', 'shocking', ',', 'in', 'reality', 'sex', 'and', 'nudity', 'are', 'a', 'major', 'staple', 'in', 'swedish', 'cinema', '.', 'even', 'ingmar', 'bergman', ',', 'arguably', 'their', 'answer', 'to', 'good', 'old', 'boy', 'john', 'ford', ',', 'had', 'sex', 'scenes', 'in', 'his', 'films.', '<', 'br', '/', '>', '<', 'br']\n",
      "\n",
      "Token IDS:\n",
      "[2, 3, 2, 4, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 16, 18, 19, 20, 21, 22, 2, 23, 24, 14, 25, 19, 16, 18, 26, 27, 28, 29, 30, 16, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 10, 42, 43, 44, 45, 46, 2, 47, 48, 33, 49, 35, 50, 51, 52, 53, 54, 55, 52, 53, 54, 55, 12, 56, 57, 58, 59, 40, 60, 61, 62, 63, 64, 65, 66, 67, 33, 68, 69, 70, 71, 72, 73, 74, 21, 75, 70, 67, 33, 76, 77, 78, 33, 79, 80, 81, 10, 82, 83, 84, 12, 85, 1, 86, 72, 87, 88, 89, 90, 91, 12, 92, 93, 94, 95, 89, 21, 12, 96, 97, 74, 21, 98, 99, 100, 94, 101, 102, 10, 103, 72, 104, 105, 83, 106, 37, 70, 107, 108, 109, 77, 62, 110, 37, 111, 37, 94, 112, 113, 52, 53, 54, 55, 52, 53, 54, 55, 84, 114, 115, 72, 2, 4, 1, 57, 14, 116, 117, 118, 37, 35, 18, 43, 119, 74, 47, 37, 12, 108, 94, 120, 121, 122, 123, 94, 124, 98, 37, 125, 126, 16, 127, 128, 129, 130, 80, 131, 132, 133, 74, 134, 6, 1, 135, 136, 16, 137, 37, 21, 138, 108, 94, 120, 122, 40, 139, 140, 21, 61, 141, 74, 125, 142, 143, 37, 144, 104, 145, 33, 146, 147, 148, 149, 150, 37, 48, 108, 121, 21, 151, 152, 52, 53, 54, 55, 52, 53]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first example before and after tokenization\n",
    "original_text = train_data_num[0][\"text\"]  # Get original text\n",
    "tokenized_ids = train_data_num[0][\"ids\"]  # Get tokenized input IDs\n",
    "\n",
    "vocab_inv = {v: k for k, v in vocab.items()}  # Invert mapping\n",
    "tokenized_text = [vocab_inv.get(token, \"[UNK]\") for token in tokenized_ids]\n",
    "print(f\"Original Text:\\n{original_text}\\n\")\n",
    "print(f\"Tokens:\\n{tokenized_text}\\n\")\n",
    "print(f\"Token IDS:\\n{tokenized_ids}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b48828a-dcba-4639-836a-3c9265889679",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tensor = train_data_num.with_format(type=\"torch\", columns=[\"ids\", \"label\"])\n",
    "test_data_tensor = test_data_num.with_format(type=\"torch\", columns=[\"ids\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b968a0-8084-4edc-ba56-45c2e08d090c",
   "metadata": {},
   "source": [
    "## Building dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea7501-7413-4700-b09a-7ebfe3bfbf76",
   "metadata": {},
   "source": [
    "* The get_collate_fn function defines a custom collation function for PyTorch's DataLoader that processes batches of variable-length sequences. \n",
    "* Handles batches where input sequences have different lengths by:\n",
    "  * Padding sequences to match the longest sequence in the batch\n",
    "  * Properly stacking labels\n",
    "```\n",
    "[\n",
    "  {\"ids\": tensor([5, 9, 2]),     \"label\": tensor(1)},\n",
    "  {\"ids\": tensor([3, 4]),        \"label\": tensor(0)},\n",
    "  {\"ids\": tensor([7, 8, 6, 1]),  \"label\": tensor(1)}\n",
    "]\n",
    "```\n",
    "-->\n",
    "```\n",
    "{\n",
    "  \"ids\": tensor([\n",
    "      [5, 9, 2, 0],\n",
    "      [3, 4, 0, 0],\n",
    "      [7, 8, 6, 1]\n",
    "  ]),\n",
    "  \"label\": tensor([1, 0, 1])\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c949420-355d-4a27-a1ae-01041ced1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_token_index):\n",
    "    def collate_fn(batch):\n",
    "        ids = [item[\"ids\"] for item in batch]\n",
    "        labels = [item[\"label\"] for item in batch]\n",
    "\n",
    "        ids = nn.utils.rnn.pad_sequence(\n",
    "            ids, batch_first=True, padding_value=pad_token_index\n",
    "        )\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return {\"ids\": ids, \"label\": labels}\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b951b370-d8ca-4038-b412-5d23a866ddbb",
   "metadata": {},
   "source": [
    "* get_data_loader creates a PyTorch DataLoader configured with a custom collation function for handling variable-length sequences. It wraps PyTorch's standard DataLoader while incorporating the padding logic from the get_collate_fn function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac1c30ab-16fc-4468-a048-ebf401dfb7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_token_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_token_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21a32851-87a0-413b-8377-7c98ca6991ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_data_loader = get_data_loader(train_data_tensor, batch_size, pad_token_index, shuffle=True)\n",
    "test_data_loader = get_data_loader(test_data_tensor, batch_size, pad_token_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac02c4d1-61b8-4ce4-a32f-e951b3bda261",
   "metadata": {},
   "source": [
    "## Building the Network with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0864b798-29cf-4c1e-9d17-ee603e118b1e",
   "metadata": {},
   "source": [
    "<img src=\"imgs/nbow_model.png\" width=\"800\" height=\"800\" />\n",
    "A simple neural network for text classification that treats input text as an unordered set of words (bag-of-words representation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d0cc5a7-680a-4f0e-8347-46d9c0d5a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBoW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_token_index):\n",
    "        super().__init__()\n",
    "        # Embedding layer:\n",
    "        # Maps each token ID -> embedding vector\n",
    "        # padding_idx ensures that embeddings for PAD tokens are always zero\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            padding_idx=pad_token_index\n",
    "        )\n",
    "        \n",
    "        # Fully-connected (linear) layer:\n",
    "        # Maps embedding_dim -> output_dim (e.g., number of classes)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, ids):\n",
    "        # Convert token IDs into embeddings\n",
    "        # Result shape: [batch_size, sequence_length, embedding_dim]\n",
    "        embedded = self.embedding(ids)\n",
    "        \n",
    "        # Pool across the sequence dimension using mean\n",
    "        # This collapses variable-length sequences into a single vector\n",
    "        # Result shape: [batch_size, embedding_dim]\n",
    "        pooled = embedded.mean(dim=1)\n",
    "        \n",
    "        # Apply linear layer to get class scores (logits)\n",
    "        prediction = self.fc(pooled)\n",
    "        \n",
    "        # Result shape: [batch_size, output_dim]\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d091ee43-6839-424c-959a-269d8f34ad8c",
   "metadata": {},
   "source": [
    "### Embedding Layer\n",
    "* Maps token IDs → dense vectors\n",
    "\n",
    "### Mean Pooling\n",
    "* Averages word embeddings along sequence dimension\n",
    "* Converts [batch_size, seq_len, emb_dim] → [batch_size, emb_dim]\n",
    "* Captures overall document semantics\n",
    "\n",
    "### Linear Classifier\n",
    "* Maps pooled embedding → class logits\n",
    "* Output_dim: Number of classes (e.g., 2 for binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8849f6fd-c9a4-4bb9-90e3-1ef4b2d8a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "output_dim = len(train_data_tensor.unique(\"label\"))\n",
    "\n",
    "model = NBoW(vocab_size, embedding_dim, output_dim, pad_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94a6f23f-402c-46a8-9479-31e6d1464468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,682,702 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f17a6a8-feb3-4987-96e9-6ab7784a862f",
   "metadata": {},
   "source": [
    "### Download pre-trained embedding\n",
    "* Code just downloads archive and unarchives desired dimensionality.\n",
    "* Pretrained embedding capture simularities, e.g. if words are similar then cosine is -> 1\n",
    "```\n",
    "             king    france  computer\n",
    "king     1.000000  0.345671  0.123456\n",
    "queen    0.856734  0.234567  0.098765\n",
    "prince   0.823456  0.287654  0.087654\n",
    "...          ...       ...       ...\n",
    "france   0.345671  1.000000  0.056789\n",
    "germany  0.301234  0.912345  0.043210\n",
    "paris    0.287654  0.876543  0.065432\n",
    "...          ...       ...       ...\n",
    "computer 0.123456  0.056789  1.000000\n",
    "software 0.098765  0.043210  0.876543\n",
    "keyboard 0.087654  0.032109  0.812345\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bd3a855-7f94-45a0-9719-dc147d205f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe 300d embedding ready at: glove/glove.6B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "def download_glove(dim=100, save_dir=\"glove\"):\n",
    "    assert dim in [50, 100, 200, 300], \"Invalid dimension. Must be one of [50, 100, 200, 300]\"\n",
    "    url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "    zip_path = os.path.join(save_dir, \"glove.6B.zip\")\n",
    "    target_file = f\"glove.6B.{dim}d.txt\"\n",
    "    target_path = os.path.join(save_dir, target_file)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(target_path):\n",
    "        if not os.path.exists(zip_path):\n",
    "            print(\"Downloading GloVe embeddings...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            with open(zip_path, \"wb\") as f, tqdm(\n",
    "                desc=zip_path,\n",
    "                total=total_size,\n",
    "                unit=\"B\",\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as bar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    f.write(data)\n",
    "                    bar.update(len(data))\n",
    "\n",
    "        print(\"Extracting...\")\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extract(target_file, path=save_dir)\n",
    "\n",
    "    print(f\"GloVe {dim}d embedding ready at: {target_path}\")\n",
    "    return target_path\n",
    "\n",
    "# Example usage:\n",
    "glove_path = download_glove(dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c99284c-3e37-4730-991f-071c6159da93",
   "metadata": {},
   "source": [
    "### Load embedding matrix into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c974c64e-1ba0-40a5-b447-befaa6bcdc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: torch.Size([25607, 300])\n"
     ]
    }
   ],
   "source": [
    "# Convert numerical token IDs back into words (tokens).\n",
    "# vocab = { \"[PAD]\": 0,\"[UNK]\": 1, \"the\": 2, \"cat\": 3, \"sat\": 4 }\n",
    "# itos = {0: \"[PAD]\", 1: \"[UNK]\", 2: \"the\", 3: \"cat\", 4: \"sat\" }\n",
    "itos = {idx: token for token, idx in vocab.items()}\n",
    "\n",
    "# Load GloVe embeddings from file system\n",
    "def load_glove_embeddings(filepath):\n",
    "    embeddings = {}\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]  # First word is the token\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")  # Remaining values are the embedding\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Path to GloVe file (adjust the path to your local file)\n",
    "# glove_path = \"glove.6B.300d.txt\"  # Use 300-dimensional GloVe embeddings\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n",
    "\n",
    "avg_glove_embedding = np.mean(np.array(list(glove_embeddings.values())), axis=0)\n",
    "# Create an embedding matrix for the vocabulary\n",
    "embedding_matrix = np.zeros((len(itos), embedding_dim))\n",
    "\n",
    "for idx, token in enumerate(itos):\n",
    "    if token in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[token]\n",
    "    else:\n",
    "        embedding_matrix[idx] = avg_glove_embedding  # init for unknown tokens\n",
    "\n",
    "# Convert embedding matrix to PyTorch tensor\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fa1f6ee-127a-4f2e-8285-0a78a41c59dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings loaded into the model.\n"
     ]
    }
   ],
   "source": [
    "# Convert the embedding matrix to a PyTorch tensor\n",
    "embedding_matrix = embedding_matrix.clone().detach().float()\n",
    "\n",
    "# Assign to the model's embedding layer\n",
    "model.embedding.weight.data.copy_(embedding_matrix)\n",
    "print(\"GloVe embeddings loaded into the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c389a8af-d443-488d-855b-9fe34fcbd492",
   "metadata": {},
   "source": [
    "## Building Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c2a0bf8-44f2-4276-96b3-5a7fcb17fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4442b15-b2ce-4d27-adbd-8e380934c35d",
   "metadata": {},
   "source": [
    "Adaptive Learning Rates (ADAM):\n",
    "* Adam automatically adjusts learning rates for each parameter (using momentum and adaptive scaling), which:\n",
    "* Works well with sparse gradients (common in NLP)\n",
    "* Eliminates need for manual learning rate scheduling\n",
    "* Outperforms SGD for most NLP tasks\n",
    "\n",
    "Output Layer Behavior:\n",
    "* The final layer produces raw logits (unnormalized scores)\n",
    "* This is the required input format for nn.CrossEntropyLoss\n",
    "* The loss function internally applies LogSoftmax + NLLLoss\n",
    "```\n",
    "# Equivalent to:\n",
    "log_probs = F.log_softmax(model(inputs), dim=1)\n",
    "loss = F.nll_loss(log_probs, labels)\n",
    "```\n",
    "\n",
    "Standard Practice:\n",
    "* PyTorch classification models typically output raw logits\n",
    "* This pattern is used in official tutorials and production code\n",
    "* Simplifies the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5db74-82e0-4005-a698-709948f605c4",
   "metadata": {},
   "source": [
    "### Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e453e-f2ec-41e3-b16d-a29862330c04",
   "metadata": {},
   "source": [
    "* *tqdm*: use tqdm to display a real-time progress bar during training, which provides valuable feedback about training progress, estimated time remaining, and batch processing speed. While you can use enumerate, tqdm offers significant usability advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8edb8173-2f9c-4900-99df-8c54d835be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, label):\n",
    "    return (prediction.argmax(dim=-1) == label).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c72cd0cf-042e-4fa4-bc35-5eac1a0aedfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    losses, accs = [], []\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"training...\"):\n",
    "        ids = batch[\"ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(ids)\n",
    "        loss = criterion(preds, labels)\n",
    "        acc = get_accuracy(preds, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        accs.append(acc.item())\n",
    "\n",
    "    return np.mean(losses), np.mean(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0aaad9-cb22-47ed-9dc3-bcd49dc3cf3b",
   "metadata": {},
   "source": [
    "### Test step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f9742cf-1532-436d-a356-bd89798cf82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dl, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    losses, accs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b in tqdm(dl, desc=\"evaluating...\"):\n",
    "            x, y = b[\"ids\"].to(device), b[\"label\"].to(device)\n",
    "            p = model(x)\n",
    "            l = loss_fn(p, y)\n",
    "            losses.append(l.item())\n",
    "            accs.append(get_accuracy(p, y).item())\n",
    "\n",
    "    return np.mean(losses), np.mean(accs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87557bd0-549e-40ad-9cc1-c5c8649c9ba6",
   "metadata": {},
   "source": [
    "## Model in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089921a7-0c71-4ea8-9780-97556385e619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:10<00:00,  4.52it/s]\n",
      "evaluating...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:04<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 0.685, Train Acc: 0.572 | Test Loss: 0.672, Test Acc: 0.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:10<00:00,  4.75it/s]\n",
      "evaluating...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:04<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.636, Train Acc: 0.749 | Test Loss: 0.606, Test Acc: 0.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:10<00:00,  4.73it/s]\n",
      "evaluating...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:04<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.544, Train Acc: 0.796 | Test Loss: 0.523, Test Acc: 0.791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:10<00:00,  4.72it/s]\n",
      "evaluating...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:04<00:00, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.453, Train Acc: 0.838 | Test Loss: 0.458, Test Acc: 0.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:10<00:00,  4.74it/s]\n",
      "evaluating...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:04<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.385, Train Acc: 0.866 | Test Loss: 0.415, Test Acc: 0.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:10<00:00,  4.68it/s]\n",
      "evaluating...:  45%|████████████████████████████████████████████                                                      | 22/49 [00:01<00:02, 12.06it/s]"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "metrics = collections.defaultdict(list)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train(train_data_loader, model, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(test_data_loader, model, criterion, device)\n",
    "\n",
    "    metrics[\"train_losses\"].append(train_loss)\n",
    "    metrics[\"train_accs\"].append(train_acc)\n",
    "    metrics[\"test_losses\"].append(test_loss)\n",
    "    metrics[\"test_accs\"].append(test_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch} | \"\n",
    "        f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.3f} | \"\n",
    "        f\"Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.3f}\"\n",
    "    )\n",
    "torch.save(model.state_dict(), \"nbow.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a20a4e-34de-4893-896c-22f5dc87a7ae",
   "metadata": {},
   "source": [
    "### Performance during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2df3ed1-9e61-4692-8555-5d50f12ba864",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(metrics[\"train_losses\"], label=\"train loss\")\n",
    "ax.plot(metrics[\"test_losses\"], label=\"valid loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_xticks(range(n_epochs))\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd0b81f-729b-4227-925d-828e8f82b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(metrics[\"train_accs\"], label=\"train accuracy\")\n",
    "ax.plot(metrics[\"test_accs\"], label=\"test accuracy\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_xticks(range(n_epochs))\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6755fc-1b67-44d4-8bd5-bdd4ed481a14",
   "metadata": {},
   "source": [
    "## Usage of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbdd83d-cff1-4f7c-9cfe-404125dc11f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"nbow.pt\", weights_only=True))\n",
    "\n",
    "test_loss, test_acc = evaluate(test_data_loader, model, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43dd77-4e3f-4f4c-b3eb-edbbe15e681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"test_loss: {test_loss:.3f}, test_acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ae29b1-c14f-4b43-8fbc-0b4384a20a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, vocab, device):\n",
    "    \"\"\"\n",
    "    Predict sentiment (positive/negative) for a single text string.\n",
    "\n",
    "    Returns:\n",
    "        predicted_class (int): 0 = negative, 1 = positive\n",
    "        predicted_probability (float): confidence of the prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input text\n",
    "    tokens = get_tokens(text)\n",
    "\n",
    "    # Convert tokens to vocabulary indices (use [UNK] or 0 for unknown words)\n",
    "    ids = [vocab.get(tok, vocab.get(\"[UNK]\", 0)) for tok in tokens]\n",
    "\n",
    "    # Convert to tensor and add batch dimension: [1, seq_len]\n",
    "    ids = torch.LongTensor(ids).unsqueeze(0).to(device)\n",
    "\n",
    "    # Disable gradient tracking for inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(ids).squeeze(0)          # [num_classes]\n",
    "        probs = torch.softmax(logits, dim=-1)   # convert to probabilities\n",
    "\n",
    "    # Get predicted class and its probability\n",
    "    pred_class = logits.argmax().item()\n",
    "    pred_prob = probs[pred_class].item()\n",
    "\n",
    "    # Map class index to label\n",
    "    sentiment = \"pos\" if pred_class == 1 else \"neg\"\n",
    "\n",
    "    print(f\"Sentiment: {sentiment} ({pred_prob:.4f})\")\n",
    "\n",
    "    return pred_class, pred_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0ac4e-b71e-4595-b0c1-4f1c23abdf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is terrible!\"\n",
    "\n",
    "predict_sentiment(text, model, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e37fe-403a-4364-996e-34ef6e31ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is great!\"\n",
    "\n",
    "predict_sentiment(text, model, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6073da-fb6d-40d9-88b5-56ba73395a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is not terrible, it's great!\"\n",
    "\n",
    "predict_sentiment(text, model, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecdd847-394f-4a5f-9f0a-508e10f2ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is not great, it's terrible!\"\n",
    "\n",
    "predict_sentiment(text, model, vocab, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
